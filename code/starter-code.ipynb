{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4.3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup your imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Figure out what newsgroup each block of text belongs to and create list of 20 different categories\n"
     ]
    }
   ],
   "source": [
    "print \"Goal: Figure out what newsgroup each block of text belongs to and create list of 20 different categories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Steps To Take:\n",
    "\n",
    "# 1) Gather and clean data\n",
    "\n",
    "# 2) Turn text into numbers\n",
    "\n",
    "# 3) Plot numbers\n",
    "\n",
    "# 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pull the training set from the newsgroup data\n",
    "The data has 20 different categories. Try to shrink down to smaller number of groups according to the definition here:\n",
    "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove =['headers','footers','quotes'])\n",
    "\n",
    "x = newsgroups_train.data # Text data (emails, etc)\n",
    "y = newsgroups_train.target #Newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 20 different categories:\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print \"List of 20 different categories:\"\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 6, 6, 4, 6, 1, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# Reduce to 6 targets\n",
    "\n",
    "mappy = {\n",
    "    0: [0],\n",
    "    1: [1,2,3,4,5],\n",
    "    2: [6],\n",
    "    3: [7,8,9,10],\n",
    "    4: [11,12,13,14],\n",
    "    5: [15],\n",
    "    6: [16,17,18,19]\n",
    "}\n",
    "def getkey(num):\n",
    "    for x, y in mappy.items():\n",
    "        if num in y:\n",
    "            return x\n",
    "y_train = [getkey(num) for num in y_train]\n",
    "y_test = [getkey(num) for num in y_test]\n",
    "\n",
    "labels = ['alt','comp','misc','rec','sci','soc','talk']\n",
    "\n",
    "print y_train[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learn</th>\n",
       "      <th>runs</th>\n",
       "      <th>night</th>\n",
       "      <th>person</th>\n",
       "      <th>similar</th>\n",
       "      <th>room</th>\n",
       "      <th>teams</th>\n",
       "      <th>armenia</th>\n",
       "      <th>like</th>\n",
       "      <th>des</th>\n",
       "      <th>...</th>\n",
       "      <th>market</th>\n",
       "      <th>command</th>\n",
       "      <th>probably</th>\n",
       "      <th>sounds</th>\n",
       "      <th>talk</th>\n",
       "      <th>shot</th>\n",
       "      <th>week</th>\n",
       "      <th>d9</th>\n",
       "      <th>general</th>\n",
       "      <th>cover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   learn  runs  night  person  similar  room  teams  armenia  like  des  \\\n",
       "0      0     0      0       0        0     0      0        0     0    0   \n",
       "1      0     0      0       0        0     0      0        0     0    0   \n",
       "2      0     0      0       0        0     0      0        0     0    0   \n",
       "3      0     0      0       0        0     0      0        0     0    0   \n",
       "4      0     0      0       0        0     0      0        0     0    0   \n",
       "\n",
       "   ...    market  command  probably  sounds  talk  shot  week  d9  general  \\\n",
       "0  ...         0        0         0       0     0     0     0   0        0   \n",
       "1  ...         0        0         0       0     0     0     0   0        0   \n",
       "2  ...         0        0         0       0     0     0     0   0        0   \n",
       "3  ...         0        0         0       0     0     0     0   0        0   \n",
       "4  ...         0        0         0       0     0     0     0   0        0   \n",
       "\n",
       "   cover  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vectorizer of words\n",
    "v = CountVectorizer(stop_words = 'english', ngram_range = (1,2), max_features=1000)\n",
    "\n",
    "# Fit Vectorizer\n",
    "vec = v.fit(x)\n",
    "x_train = vec.transform(x_train).todense()\n",
    "x_test = vec.transform(x_test).todense()\n",
    "\n",
    "# Visualize the vectorizer (Rows are newsgroups, columns are earch word)\n",
    "df_x_train = pd.DataFrame(x_train, columns = list(v.vocabulary_))\n",
    "\n",
    "df_x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are top 50 most powerful terms in deciding news groups? (hint: treat it as a classification problem)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Madballa55/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n",
      "/Users/Madballa55/anaconda/lib/python2.7/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Using Logistic Regression and get coeficients\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "# Create model\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='sag')\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_predictions = lr.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.664702731655\n",
      "Confusion Matrix\n",
      "[[ 24  25   0  17  16   9  57]\n",
      " [  0 847   5  31  61   1   3]\n",
      " [  0  37  99  29  22   0   1]\n",
      " [  1 157   6 547  51   2  25]\n",
      " [  3 169   2  94 475   0  52]\n",
      " [  4  18   1  15  13  84  67]\n",
      " [  5  82   2  73  71  25 406]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.16      0.26       148\n",
      "          1       0.63      0.89      0.74       948\n",
      "          2       0.86      0.53      0.65       188\n",
      "          3       0.68      0.69      0.69       789\n",
      "          4       0.67      0.60      0.63       795\n",
      "          5       0.69      0.42      0.52       202\n",
      "          6       0.66      0.61      0.64       664\n",
      "\n",
      "avg / total       0.67      0.66      0.65      3734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Return accuracy\n",
    "print \"Score: \", lr.score(x_test, y_test)\n",
    "print \"Confusion Matrix\"\n",
    "conf_matrix = confusion_matrix(y_test, y_predictions)\n",
    "print conf_matrix\n",
    "\n",
    "print classification_report(y_test, y_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learn</th>\n",
       "      <th>runs</th>\n",
       "      <th>night</th>\n",
       "      <th>person</th>\n",
       "      <th>similar</th>\n",
       "      <th>room</th>\n",
       "      <th>teams</th>\n",
       "      <th>armenia</th>\n",
       "      <th>like</th>\n",
       "      <th>des</th>\n",
       "      <th>...</th>\n",
       "      <th>market</th>\n",
       "      <th>command</th>\n",
       "      <th>probably</th>\n",
       "      <th>sounds</th>\n",
       "      <th>talk</th>\n",
       "      <th>shot</th>\n",
       "      <th>week</th>\n",
       "      <th>d9</th>\n",
       "      <th>general</th>\n",
       "      <th>cover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000570</td>\n",
       "      <td>0.007567</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>0.063545</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>-0.019603</td>\n",
       "      <td>-0.000860</td>\n",
       "      <td>-0.013873</td>\n",
       "      <td>0.026469</td>\n",
       "      <td>-0.000928</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.008796</td>\n",
       "      <td>-0.042217</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>-0.020511</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>0.005382</td>\n",
       "      <td>-0.019449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>-0.045802</td>\n",
       "      <td>0.036949</td>\n",
       "      <td>0.035984</td>\n",
       "      <td>-0.061782</td>\n",
       "      <td>-0.075862</td>\n",
       "      <td>-0.047421</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>-0.032135</td>\n",
       "      <td>-0.034048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069203</td>\n",
       "      <td>0.021646</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.034097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010186</td>\n",
       "      <td>-0.008227</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>-0.005627</td>\n",
       "      <td>-0.031964</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.006774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.004532</td>\n",
       "      <td>0.023355</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>-0.004519</td>\n",
       "      <td>-0.003917</td>\n",
       "      <td>-0.002980</td>\n",
       "      <td>0.008199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047211</td>\n",
       "      <td>-0.026451</td>\n",
       "      <td>-0.013795</td>\n",
       "      <td>-0.015364</td>\n",
       "      <td>0.180301</td>\n",
       "      <td>0.019292</td>\n",
       "      <td>0.017791</td>\n",
       "      <td>-0.020591</td>\n",
       "      <td>0.035043</td>\n",
       "      <td>0.022053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.035638</td>\n",
       "      <td>-0.026250</td>\n",
       "      <td>-0.009564</td>\n",
       "      <td>-0.011868</td>\n",
       "      <td>-0.019396</td>\n",
       "      <td>0.015293</td>\n",
       "      <td>-0.011865</td>\n",
       "      <td>-0.012997</td>\n",
       "      <td>-0.010871</td>\n",
       "      <td>-0.016693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046479</td>\n",
       "      <td>-0.036416</td>\n",
       "      <td>-0.029279</td>\n",
       "      <td>-0.025776</td>\n",
       "      <td>-0.049712</td>\n",
       "      <td>0.061001</td>\n",
       "      <td>0.034011</td>\n",
       "      <td>-0.007011</td>\n",
       "      <td>-0.041070</td>\n",
       "      <td>-0.036123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      learn      runs     night    person   similar      room     teams  \\\n",
       "0 -0.000570  0.007567  0.002878  0.003168  0.005718  0.006668  0.004404   \n",
       "1 -0.008796 -0.042217  0.002859  0.001681  0.007635 -0.020511  0.009618   \n",
       "2  0.069203  0.021646  0.001001  0.001164  0.004099  0.004834  0.002242   \n",
       "3 -0.004532  0.023355  0.004628  0.007347  0.000915 -0.005811 -0.004519   \n",
       "4 -0.035638 -0.026250 -0.009564 -0.011868 -0.019396  0.015293 -0.011865   \n",
       "\n",
       "    armenia      like       des    ...       market   command  probably  \\\n",
       "0  0.004680  0.004030 -0.001678    ...     0.030704  0.063545  0.004834   \n",
       "1  0.008233  0.005382 -0.019449    ...     0.018270 -0.045802  0.036949   \n",
       "2  0.004393  0.004638  0.034097    ...    -0.010186 -0.008227  0.006129   \n",
       "3 -0.003917 -0.002980  0.008199    ...    -0.047211 -0.026451 -0.013795   \n",
       "4 -0.012997 -0.010871 -0.016693    ...    -0.046479 -0.036416 -0.029279   \n",
       "\n",
       "     sounds      talk      shot      week        d9   general     cover  \n",
       "0  0.004141 -0.019603 -0.000860 -0.013873  0.026469 -0.000928  0.011300  \n",
       "1  0.035984 -0.061782 -0.075862 -0.047421  0.017046 -0.032135 -0.034048  \n",
       "2  0.009851 -0.002797  0.005069 -0.005627 -0.031964  0.005325  0.006774  \n",
       "3 -0.015364  0.180301  0.019292  0.017791 -0.020591  0.035043  0.022053  \n",
       "4 -0.025776 -0.049712  0.061001  0.034011 -0.007011 -0.041070 -0.036123  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DF with coefecients and words\n",
    "df_coef = pd.DataFrame(lr.coef_, columns = list(v.vocabulary_))\n",
    "\n",
    "# Sort by top words\n",
    "abs(df_coef.ix[6]).sort_values(ascending = False)[0:10]\n",
    "\n",
    "df_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Madballa55/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 11314]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f397cd24f5f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mselected_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mkbest_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_Xbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkbest_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 11314]"
     ]
    }
   ],
   "source": [
    "# Find top 50 using K-Best \n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=50)\n",
    "selected_data = selector.fit_transform(x, y)\n",
    "kbest_columns = x.columns[selector.get_support()]\n",
    "df_Xbest = pd.DataFrame(selected_data, columns=kbest_columns)\n",
    "\n",
    "df_Xbest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create the Truncated Singular Value Decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train, Test, Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .33, random_state = 42)\n",
    "\n",
    "# Create vectorizer of words\n",
    "v = CountVectorizer(stop_words = 'english', ngram_range = (1,2), token_pattern='[a-zA-z]{3,50}', max_features=1000)\n",
    "\n",
    "# Fit Vectorizer\n",
    "x_vec = v.fit(x)\n",
    "x_train = vec.transform(x_train)\n",
    "x_test = vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5703765   0.06492919  0.03048025  0.02542723  0.02096249  0.01442626\n",
      "  0.01354996  0.01176197  0.0116698   0.01049065  0.01033613  0.00886223\n",
      "  0.00765617  0.00737933  0.00639738  0.00537683  0.00518967  0.00481976\n",
      "  0.00407646  0.00397765  0.00386957  0.00362885  0.00337894  0.00311016\n",
      "  0.00297092  0.00266808  0.00250592  0.00232504  0.00222912  0.00220091\n",
      "  0.00207813  0.0020416   0.00200061  0.00196643  0.00195494  0.00187375\n",
      "  0.00184024  0.00180891  0.00173396  0.00165543  0.00155811  0.00149014\n",
      "  0.00143859  0.00140879  0.00132653  0.00122297  0.0011927   0.00112166\n",
      "  0.00109169  0.00106718]\n",
      "Accuracy:  0.898905766593\n"
     ]
    }
   ],
   "source": [
    "tsvd = TruncatedSVD(n_components=50, n_iter=7, random_state=42)\n",
    "tsvd.fit(x_train)\n",
    "\n",
    "print tsvd.explained_variance_ratio_\n",
    "\n",
    "print \"Accuracy: \", sum(tsvd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Setup your k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=7, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = tsvd.transform(x_train)\n",
    "\n",
    "from sklearn import cluster\n",
    "k = 7\n",
    "kmeans = cluster.KMeans(n_clusters=k)\n",
    "kmeans.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_predictions = kmeans.labels_\n",
    "labels_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fit the vectorizer and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are top 50 most useful terms based on article itself? Are those terms similar to the top 50 from step 2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cummulative variace from the terms. x-axis: number of components; y-axis: cummulative variance. \n",
    "Based on the plot, decide how many principle components you need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Fit the kmeans (Question: in this case, do you recommend running K-means without dimension reduction?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Print out your centroids. Look at the value for each centroid. Does each centroid represent a news group as expected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Check the performance of our kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix. Hint: create a map to translate the label between k-means clustering and the original target (newsgroups_train.target). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Repeat the lab with:\n",
    "- varying values of \"k\" \n",
    "- trying a different way to pick starting centroids ('k-means++' is the default method for centroids). For example, pick one point from each newsgroup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
